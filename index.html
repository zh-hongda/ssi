<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Spatial and Semantic Sensitive Imitation Learning Model for Robotic Manipulation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Hongda Zhang</a>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Yi Liu</a>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Bingsheng Wei</a>
                  </span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Chun Ouyang</a>
                  </span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zhongxue Gan</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">College of Intelligent Robotics and Advanced Manufacturing, Fudan University,
Shanghai, China<br></span>
                    <span class="eql-cntrb"><small><br><sup></sup></small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/zh-hongda/ssi/tree/master/code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Imitation learning methods have shown outstanding performance in the field of intelligent robotic services. These approaches typically utilize RGB images and robot proprioceptive information as inputs to predict subsequent actions through end-to-end models. Such methods extract features using pre-trained image encoders to assist model decision-making. However, it does not guarantee that the model correctly understands spatial positions and semantic information, which may lead to incorrect inferences about object relationships and insufficient focus on task-critical targets, thereby limiting further performance improvement. To address this issue, this paper proposes an improved imitation learning method, the Spatial and Semantic Sensitive Imitation Learning Model for Robotic Manipulation (SSI). This method uses depth maps generated by depth estimation and semantic maps obtained through instance segmentation, combined with robot proprioceptive information as input, to enhance the model's environmental understanding and action prediction capabilities. Depth maps provide spatial information about the scene, enabling the model to capture spatial relationships between objects; semantic maps explicitly label object categories and boundaries in the scene, helping the model focus on task-relevant targets. By integrating geometric and semantic information, this approach effectively enhances the model's comprehensive environmental perception capabilities. Furthermore, we utilize the pre-trained general robotic operation method, OpenVLA, to generate demonstration data, training the SSI model with spatial and semantic awareness. 
Experimental results show that, compared to existing methods, the proposed approach demonstrates higher accuracy and robustness of action prediction in various task scenarios, providing a new perspective for the application of imitation learning in complex environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero">
    <div class="container is-max-widescreen">
        <div class="columns is-centered">
            <div class="column is-full-width">
                <h2 class="title is-3">InterACT</h2>

                <img src="./static/images/model_overview.png" class="interpolation-image">
                <br>
                <br>

                <div class="content has-text-justified">
                    <p>
                      Overview of Spatial and Semantic Sensitive Imitation Learning Model (SSI). This model predicts 7-dimensional robotic arm actions based on input images and robot states. The architecture are divided into two main parts: the lower part illustrates the process of generating and distilling successful episodes using the general pre-trained robotic arm operation model (OpenVLA) to construct a demonstration dataset, while the upper part presents the model structure. The images and robot states are processed through MDAM Module before being fed into the diffusion model. The diffusion model consists of a Transformer Encoder and a Transformer Decoder: the encoder generates latent embeddings, while the decoder takes noisy actions as input, performs cross-attention with the latent embeddings produced by the encoder, and ultimately outputs the predicted noise.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Multi-Modal Dynamic Attention Modulation Module</h2>
            <p>
              <img src="./static/images/Dynamic_Attention_Feature_Compression_Module.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 100%; height: auto;">
            </p>
            <p>
              Overview of Multi-Modal Dynamic Attention Modulation Module. To facilitate the dynamic adaptation of the fused visual representation to the robot's embodiment, we propose the MDAM module. This module utilizes the robot's state as a dynamic modulation signal to guide a cross-attention mechanism, enabling adaptive fusion of semantic and depth features according to the robot’s current embodiment. The robot's state inherently encodes critical information regarding its pose and motion, which directly impacts task context and visual attention priorities. For instance, when the robot arm is in proximity to an object, it likely indicates an impending manipulation task; conversely, if the joint angles suggest a placement posture, the model should shift attention towards potential placement targets. Additionally, a dedicated Feature Modulation Branch compresses the fused features into a compact representation, ensuring the retention of essential task-relevant information while significantly reducing computational overhead.
            </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
    <div class="container is-max-desktop">
        <div class="columns is-centered">
            <div class="column is-full">
                <div class="content">
                    <div class="level-set has-text-justified">
                        <p>
                            To comprehensively evaluate the influence of different input modalities and the MDAM module on the model's performance, we conducted a series of ablation experiments in the simulation environment of the "insert insulated terminal" task. Specifically, we compared the model's performance across five configurations: (1) the baseline model; (2) using only semantic features; (3) using only depth features; (4) using both semantic and depth features without the MDAM module; and (5) using both semantic and depth features with the MDAM module. Each configuration was subjected to 20 repeated experiments, and the average success rates of three critical stages were recorded: ``Touched Success Rate'', ``Alignment Success Rate'', and ``Insertion Success Rate''.
                        </p>
                    </div>
                </div>

            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <img src="./static/images/ablation_result.png" alt="Main results of our method ProbeGen">
        </div>
    </div>
</section>

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/stack_cup.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/sim-insulated_terminal_0.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video9">
          <video poster="" id="video9" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/eval_insetion.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video10">
          <video poster="" id="video10" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/eval_ram.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video10">
          <video poster="" id="video10" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/eval_transfoer_cube.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/put_cup_into_sink.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/sim-insulated_terminal_1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video5">
          <video poster="" id="video5" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/sim-insulated_terminal_2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video6">
          <video poster="" id="video6" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/put_cup_into_sink_2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video7">
          <video poster="" id="video7" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/stack_cup_2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video8">
          <video poster="" id="video8" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/put_lid_on_pot.mp4"
            type="video/mp4">
          </video>
        </div>
    </div>
  </div>
</section>
<!-- End video carousel -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>Coming soon</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
